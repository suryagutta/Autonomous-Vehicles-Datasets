# Autonomous Vehicles Datasets for Machine Learning
Comprehensive list of Autonomous Vehicles Datasets (papers and dataset download links) with multiple sensor modalities (LiDAR, RADAR, Stereo Camera, Thermal Camera etc.)

A wide variety of sensors are used in autonomous vehicles. The diversity of sensing modalities helps in different weather conditions. The following is a popular list of autonomous driving datasets which have been published up to date.

| Name | Published Year | Sensor Type(s) | Recording Area(s) | Description | Dataset Download and Paper(s) |
| :--- | :--- | :--- | :--- | :--- | :--- |
| ONCE Dataset (One millioN sCenEs)- Huawei Corp. | 2021 | Camera, LiDAR | China | ONCE(One millioN sCenEs) dataset can be used for 3D object detection in the autonomous driving scenario. The ONCE dataset consists of 1million LiDAR scenes and 7 million corresponding camera images. The data is selected from 144 driving hours, which is 20x longer than the largest 3D autonomous driving dataset available (e.g. nuScenes and Waymo), and it is collected across a range of different areas, periods and weather conditions. It has 15k fully annotated scenes with 5 classes (Car, Bus, Truck, Pedestrian, Cyclist). In the ONCE dataset, there are 3 weather conditions, i.e., sunny, cloudy, rainy, and 4 time periods, i.e., morning, noon, afternoon, night, for every labeled and unlabeled scene. The information, i.e., weather, period, timestamp, pose, calibration, annotations, are in a single JSON file for each scene. | [Dataset](https://once-for-auto-driving.github.io/) , [Paper](https://arxiv.org/pdf/2106.11037.pdf) |
| All-In-One Drive (AIODrive) |  2020 | Camera, LiDAR, Radar | NA | A Large-Scale Comprehensive Perception Dataset with High-Density Long-Range Point Clouds. It is a large-scale synthetic dataset that provides comprehensive sensors, annotations, and environmental variations. It has <ul><li> Eight sensor modalities (RGB, Stereo, Depth, LiDAR, SPAD-LiDAR, Radar, IMU, GPS)</li><li> Annotations for all mainstream perception tasks (e.g., detection, tracking, trajectory prediction, segmentation, depth estimation)</li><li> Rare driving scenarios such as adverse weather and lighting, crowded scenes, high-speed driving, violation of traffic rules, and accidents</li></ul> It has high-density long-range point clouds for LiDAR and SPAD-LiDAR sensors, about ten times denser and larger sensing range than Velodyne-64. | [Dataset](http://www.aiodrive.org/download.html), [Paper](https://www.xinshuoweng.com/papers/AIODrive/arXiv.pdf) |
| Ford Multi-AV Seasonal dataset |  2020 | Camera, LiDAR | United States (Michigan) | The multi-agent seasonal dataset was collected by a fleet of Ford autonomous vehicles on different days and times during 2017–18. The vehicles were manually driven on a route in Michigan that included a mix of driving scenarios, including the Detroit Airport, freeways, city-centers, university campus, and suburban neighborhood. The dataset has seasonal variation in weather, lighting, construction, and traffic conditions experienced in dynamic urban environments. | [Dataset](https://avdata.ford.com/downloads/default.aspx), [Paper](https://arxiv.org/pdf/2003.07969.pdf) |
| Dense Depth for Autonomous Driving (DDAD) — Toyota Research Institute |  2020 | Camera, LiDAR | United States (San Francisco, Bay Area, Cambridge, Detroit, Ann Arbor) and Japan (Tokyo, Odaiba) | DDAD is a new autonomous driving benchmark from TRI (Toyota Research Institute) for long range (up to 250m) and dense depth estimation in challenging and diverse urban conditions. It contains monocular videos and accurate ground-truth depth (across a full 360 degree field of view) generated from high-density LiDARs mounted on a fleet of self-driving cars operating in a cross-continental setting. | [Dataset](https://github.com/TRI-ML/DDAD/blob/master/README.md) , [Paper](https://arxiv.org/pdf/1905.02693.pdf) |


